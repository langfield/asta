+----+
|asta|
+----+

Shape annotations for numpy arrays and pytorch/tensorflow tensors.

Usage
-----
This library defines two subscriptable classes, ``Array`` and ``Tensor``, whose
purpose is to facilitate type and/or shape annotations of functions which take
as argument(s) numpy ndarrays or pytorch tensors. It also adds a decorator,
``@typechecked``, which implements toggleable static type enforcement for the
classes described above.

Examples
--------
Asta supports checking dtypes and shapes:

>>> def fn(arr: Array[float, 1, 2, 3]):
>>>     pass

(Allows np.ndarrays with dtype ``np.float64`` and shape ``(1, 2, 3)``)

Or dtypes only:

>>> def fn(arr: Array[float]):
>>>     pass

(Allows np.ndarrays with dtype ``np.float64``)

Or shapes only:

>>> def fn(arr: Array[1, 2, 3]):
>>>     pass

(Allows np.ndarrays with shape ``(1, 2, 3)``)

To specify scalar arrays/tensors, use ``()`` or ``Scalar``:

>>> def fn(arr: Array[int, ()]):
>>>     pass

(Allows scalar np.ndarrays of type ``np.int64``)

You can also pass ellipses and wildcard dimensions:

>>> def fn(arr: Array[str, 1, 2, ..., 3]):
>>>     pass

(Allows unicode ``np.dtype("<U")`` np.ndarrays whose first two dimensions have
size 1 and 2, and whose last dimension has size 3.)

>>> def fn(arr: Array[str, 1, 2, -1, 3]):
>>>     pass

(Allows unicode ``np.dtype("<U")`` np.ndarrays with shape ``(1,2,*,3)``,
where ``*`` can be any positive integer)

The following gives an example of using the ``@typechecked`` decorator to
enforce torch tensor shapes and dtypes at runtime. The function ``kl`` will
raise a TypeError if called with inputs which have any dtype other than
``torch.float32``, or any shape other than ``(8, 64)``.

A runnable example is given in ``example.py`` in the repository root.

>>> import os
>>> import torch.nn.functional as F
>>> from asta import Tensor, typechecked
>>>
>>> os.environ["ASTA_TYPECHECK"] = "1"
>>>
>>>
>>> @typechecked
>>> def kl(t_1: Tensor[float, 8, 64], t_2: Tensor[float, 8, 64]) -> Tensor[float, ()]:
>>>     """ Computes the KL divergence of two FloatTensors of shape ``(8, 64)``. """
>>>     divergence = F.kl_div(t_1, t_2, reduction="sum")
>>>     return divergence

Subscript arguments
-------------------
The valid subscript arguments for ``Array`` and ``Tensor`` are as follows:

    Types
    -----

        Array
        -----
        1. Any python type from the following list:
            a. int
            b. float
            c. bool
            d. complex
            e. bytes
            f. str
            g. datetime
            h. timedelta
        2. Any numpy dtype, e.g. ``np.int64``.
        3. Omitted (no argument passed).

        Tensor
        ------
        1. Any python type from the following list:
            a. int
            b. float
            c. bool
            d. bytes
        2. Any ``torch.Tensor``-supported torch dtype, e.g. ``torch.int64``.
        3. Omitted (no argument passed).

    Shapes
    ------
    1. Nonnegative integers.
    2. ``-1``, a wildcard for any positive integer size.
    3. Ellipses (``...``), a placeholder for any contiguous sequence of
    positive integer sizes, including zero-length sequences.
    4. ``()`` or ``Scalar``, which both indicate a scalar array or tensor.
    These are interchangeable.
    5. Omitted (no argument passed).

Shape constraints and best practices
------------------------------------
There is a key difference between the way scalar values are handled in numpy
and the way they are handled in torch. Consider an array/tensor of shape
``(2,2,2)``. When indexing the first element along each dimension, which should
be scalar, we call ``a[0,0,0]``, where ``a`` is our array. We do the same to a
tensor ``t``, and assign the result to variables ``x`` and ``y``, respectively:

>>> a = np.zeros((2,2,2))
>>> t = torch.zeros((2,2,2))
>>> x = a[0,0,0]
>>> y = t[0,0,0]

What are the types of ``x`` and ``y``?

>>> type(x)
<class 'numpy.float64'>
>>> type(y)
<class 'torch.Tensor'>

Interestingly enough, while ``a`` is of type ``np.ndarray``, ``x`` is of type
``np.float64``, a subclass of float, while both ``t`` and ``y`` are tensors.
Note that ``x`` is not an array:

>>> isinstance(x, np.ndarray)
False
>>> isinstance(x, float)
True

And ``y`` is not a float:

>>> isinstance(y, torch.Tensor)
True
>>> isinstance(y, float)
False

Asta does not attempt to rectify this discrepancy, and so the behavior of
``Array`` and ``Tensor`` when it comes to scalars is slightly different. In the
above, ``x`` is not an instance of ``Array``, while ``y`` is an instance of
``Tensor``, even though they are indexed in an identical manner.

Use of the ellipsis placeholder (``...``) in asta is meant to mirror its usage
in numpy/torch indexing. This is why we allow ``...`` to take the place of an
emtpy shape. Note that indexing scalar arrays with an ellipsis returns the
array unchanged:

>>> np.zeros(())
array(0.)
>>> a = np.zeros(())
>>> a[...]
array(0.)

And adding an Ellipsis anywhere in an already-filled index tuple will return a
scalar array with the expected value:

>>> a = np.zeros((2,2,2))
>>> a[0,0,0]
0.0
>>> a[0,0,0,...]
array(0.)
>>> a[0,0,...,0]
array(0.)
>>> a[...,0,0,0]
array(0.)

In contrast, we take the ``-1`` wildcard argument to represent only a single
positive integer shape element. So if you wanted to all arrays with shape
``(1,*,1)``, where ``*`` is nonempty, i.e. don't match arrays of shape
``(1,1)``, but do match any of the following:

1. ``(1, 1, 1)``
2. ``(1, 2, 1)``
3. ``(1, 2, 3, 4, 5, 1)``

You would use ``Array[1,-1,...,1]``.


Performance
-----------
The runtime checking functionality of asta is NOT meant to be used in
situations where performance/speed is critical. Furthermore, use of the values
of type hints within python code, which ``@typechecked`` decorator relies on,
is not recommended; the ability to type hint in python is meant to be just
that, a hint. The usefulness of using the decorator is as a debugging or
testing step when working on large, complicated models or workflows. Many of
the native numpy and pytorch array/tensor functions allow arbitrary shaped
inputs, and it is easy for a malformed shape to pass unnoticed, with no effects
other than poor downstream performance or results. Asta is meant to be a crude
aide in dealing with this common problem, but by no means a comprehensive one.

This having been said, the ``isinstance()`` checks used are relatively cheap,
and shouldn't cause a serious slowdown outside of exceptional cases.

The recommended usage of this library would be to annotate all critical
functions which take or return ndarrays/tensors, and decorate them with
``@typechecked``. One could then add a CI test which sets the
``ASTA_TYPECHECK`` environment variable to ``1`` and runs a sample workflow.
Any incorrect dtypes or malformed shapes will raise a TypeError, and typechecks
which pass will print to stdout. This behavior is intentional, and meant to
help researchers avoid silent performance degradation due to leaving the
environment variable set, which will cause a slight slowdown which would
otherwise occur silently.

Todo
----
- Add ``# type: ignore`` comments in test files. The ``[type-arg]`` and
  ellipses errors will be ignored when the package is installed. They just need
  to be silenced within the package itself. (DONE)
- Delete ``demo.py``. (DONE)
- Implement ``-1`` wildcard shape element suppport. (DONE)
- Add tests for ``Tensor``. (DONE)
- Write examples in README. (DONE)
- Add tests for empty arrays and tensors. (DONE)
- Consider making Ellipses only match empty shapes and positive integer shapes,
  but not zero-valued shapes. This would be done under the assumption that most
  people are not interested in working with empty arrays/tensors. And if they
  are, they can use the ``.size`` attribute for an easy check. (DONE)
- Add reprs. (DONE)
- Fix tensor strategy. (DONE)
- Add an option to disable typechecked decorator (default=disabled, ``.astarc``
  file).
- Add environment variable for typechecking. (DONE)
- Add tests for ``@typechecked``. (DONE)
- Consider changing name of decorator to ``@shapechecked``. (NO)
- Consider dropping the ``Scalar`` object. The less unfamiliar objects, the
  better. (NO)
- Add more descriptive error if you pass torch dtype for an Array or numpy
  dtype for a Tensor. (DONE)
- Add ``CudaTensor`` class.
- Consider adding support for arbitrary shape/type constraints as subscript
  arguments. They would be of the form:

    >>> def fn(shape: Tuple[int, ...]) -> bool:
    >>>     n: int
    >>>     for i, elem in enumerate(shape):
    >>>         if i == 0:
    >>>             n = elem
    >>>         elif elem == 2 * n:
    >>>             n = elem
    >>>         else:
    >>>             return False
    >>>     return True

  The above constraint would only pass for shapes ``(n, 2n, 4n, 8n...)``.
  To enforce it, you would use ``Array[fn]``.
- Consider allowing instantiation to support kwargs. (DETAILED BELOW)
- Consider adding a delimiter in the typecheck decorator output to distinguish
  the typecheck passes between different functions and signatures. Something
  like ``===============<asta.typechecked_function()>===============``. (DONE)
- Consider adding support for tensorflow.
- Consider adding support for passing ``()`` as in ``Array[int, ()]`` to denote
  scalar arrays instead of ``Scalar`` or ``None``. (DONE)
- Write tests for ``Scalar``.
- Write analogues of new ``Array`` tests for ``Tensor``. (DONE)
- Consider reserving ``None`` shape and type for unintialized shape globals.
  (NO)
  Attempting to typecheck with them will raise an error warning that they are
  uninitialized. Then you can set your dim variable defaults in config module
  all to ``None``. (DONE)
- Add uninitialized dimension size error to decorator. See preceding note.
  (DONE)
- Add section in README about using ``asta.dims`` for programmatically-set
  dimension sizes in shape annotations.
- Fix base class of ``_ArrayMeta`` and ``_TensorMeta`` so that type ignore is
  not needed in ``decorators.py``.
- Consider removing library-specific metaclasses.
- Consider making ``parse_subscript`` a classmethod of ``SubscriptableMeta``.
  (NO)
- Remove torch, tensorflow from requirements. You should be able to ues
  ``Array`` without torch installed. (DONE)
- When you try to import Tensor when torch is not installed, no error should be
  raised. But when you then try to use/subscript Tensor, an error should be
  raised. So it should still be a subscriptable type, but the subscription
  method should try to import torch. (DONE)
- Consider adding a ``soft`` mode where the decorator only prints errors
  instead of raising them, so that a user could see and correct all of them at
  once. (DONE)
- Consider adding support for checking arbitrary attributes on arrays and
  tensors. For example, if working on a reinforcement learning problem, and you
  wanted to make sure all tensors have the same timestep index, you could seta
  ``<torch.Tensor>.timestep: int`` attribute and then assert that they all
  match at function call time.
- This could have syntax like:

    >>> def product(
    >>>     t1: Tensor(float, 1,2,3, timestep=K),
    >>>     t2: Tensor(float, 1,2,3, timestep=K + 1),
    >>> ) -> Tensor(float, 1,2,3):

  This would require implementing an ``__init__()`` for array classes, which
  would just return an instance of the metaclass with the appropriate class
  variables set. Alternatively, we could use a dictionary:

    >>> def product(
    >>>     t1: Tensor[float, 1,2,3, {"timestep": K}],
    >>>     t2: Tensor[float, 1,2,3, {"timestep": K + 1}],
    >>> ) -> Tensor[float, 1,2,3]:

  This is much better because it doesn't break the convention of using
  subscriptable objects for annotations.

- This would also require an implementation of non-contant shapes/variables.
  For example, if you know all your tensors will have length ``k``, but ``k``
  is variable, then you import ``K`` from some asta namespace. This will return
  a placeholder-like object with no set value. At typecheck time, it will be
  inferred from the first argument checked which makes use of this variable.
  The typechecker will assert that all other instances of ``K`` have the same
  value as this initialized one, and if soft checking is enabled, it will print
  out all of them along with an error message.


Acknowledgements
----------------
- Based on the excellent 'nptyping' package by Ramon Hagenaars.
- Thanks to Michael Crawshaw (@mtcrawshaw) for helpful comments on natural
  shape constraints and handling of 0-valued dimension sizes.
